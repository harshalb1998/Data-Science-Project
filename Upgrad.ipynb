{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087dac27-0b7e-4af7-94e1-ae697998068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.py\n",
    "from src import data_preprocessing, feature_engineering, model_training, model_validation, save_model, deployment_plan\n",
    "\n",
    "def main():\n",
    "    # Step 1: Data Preprocessing\n",
    "    data = data_preprocessing.load_data('data/AnomaData.xlsx')\n",
    "    data = data_preprocessing.clean_data(data)\n",
    "\n",
    "    # Step 2: Feature Engineering\n",
    "    X, y = feature_engineering.engineer_features(data)\n",
    "\n",
    "    # Step 3: Train/Test Split\n",
    "    X_train, X_test, y_train, y_test = model_training.split_data(X, y)\n",
    "\n",
    "    # Step 4: Model Training\n",
    "    rf_model, xgb_model = model_training.train_models(X_train, y_train)\n",
    "\n",
    "    # Step 5: Model Validation\n",
    "    best_model, best_params = model_validation.validate_model(rf_model, xgb_model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Step 6: Save Model\n",
    "    save_model.save_model(best_model, 'models/anomaly_detection_model.pkl')\n",
    "\n",
    "    # Step 7: Deployment Plan\n",
    "    deployment_plan.create_deployment_plan()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "src/data_preprocessing.py\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_excel(file_path)\n",
    "    return data\n",
    "\n",
    "def clean_data(data):\n",
    "    # Handle missing values by filling them with the mean\n",
    "    data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "    # Outlier detection and treatment (using IQR method)\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    data = data.apply(lambda x: x.clip(lower=x.quantile(0.05), upper=x.quantile(0.95)))\n",
    "    \n",
    "    return data\n",
    "\n",
    "src/feature_engineering.py\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def engineer_features(data):\n",
    "    # Convert date column to datetime format if exists\n",
    "    if 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek\n",
    "        data['hour'] = data['date'].dt.hour\n",
    "        data = data.drop(columns=['date'])\n",
    "\n",
    "    X = data.drop(columns=['y'])\n",
    "    y = data['y']\n",
    "\n",
    "    # Feature Selection\n",
    "    selector = SelectKBest(f_classif, k=10)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"Selected Features: {selected_features}\")\n",
    "    \n",
    "    return X_new, y\n",
    "\n",
    "src/feature_engineering.py\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def engineer_features(data):\n",
    "    # Convert date column to datetime format if exists\n",
    "    if 'date' in data.columns:\n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        data['day_of_week'] = data['date'].dt.dayofweek\n",
    "        data['hour'] = data['date'].dt.hour\n",
    "        data = data.drop(columns=['date'])\n",
    "\n",
    "    X = data.drop(columns=['y'])\n",
    "    y = data['y']\n",
    "\n",
    "    # Feature Selection\n",
    "    selector = SelectKBest(f_classif, k=10)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "    \n",
    "    print(f\"Selected Features: {selected_features}\")\n",
    "    \n",
    "    return X_new, y\n",
    "\n",
    "src/model_training.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_models(X_train, y_train):\n",
    "    # Train RandomForest\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Train XGBoost\n",
    "    xgb_model = XGBClassifier(random_state=42)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    return rf_model, xgb_model\n",
    "\n",
    "src/model_validation.py\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def validate_model(rf_model, xgb_model, X_train, y_train, X_test, y_test):\n",
    "    # Evaluate RandomForest\n",
    "    y_pred_rf = rf_model.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "    rf_classification_report = classification_report(y_test, y_pred_rf)\n",
    "    print(f\"RandomForest Accuracy: {rf_accuracy}\")\n",
    "    print(rf_classification_report)\n",
    "\n",
    "    # Hyperparameter Tuning (example with GridSearchCV for RandomForest)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "    }\n",
    "    grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best parameters and model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "src/save_model.py\n",
    "import joblib\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    joblib.dump(model, file_path)\n",
    "    print(f\"Model saved to {file_path}\")\n",
    "\n",
    "src/deployment_plan.py\n",
    "def create_deployment_plan():\n",
    "    deployment_plan = \"\"\"\n",
    "    1. Save the trained model (done above).\n",
    "    2. Develop an API using Flask or FastAPI to serve predictions.\n",
    "    3. Containerize the application using Docker.\n",
    "    4. Deploy on a cloud platform (AWS, Azure, GCP).\n",
    "    5. Set up monitoring and logging for the deployed model.\n",
    "    \"\"\"\n",
    "    print(deployment_plan)\n",
    "\n",
    "notebooks/exploratory_data_analysis.ipynb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "file_path = '../data/AnomaData.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect data\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Check data types\n",
    "data_types = data.dtypes\n",
    "print(data_types)\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = data.describe()\n",
    "print(summary_stats)\n",
    "\n",
    "# Histograms for each feature\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
